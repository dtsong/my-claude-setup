# Interaction Examples

## Example 1: Government Agriculture Engagement

**Consultant input:**
> "We've been asked by the Ministry of Agriculture in Ghana to help them evaluate technology options for reducing post-harvest losses. They're losing about 35% of maize and rice production between harvest and market. The Permanent Secretary wants a recommendation by March."

**Step 2 response:**
> Based on your description, I have a few targeted questions. Skip any you can't answer yet.
>
> 1. **Current tracking**: Does the ministry currently use any digital systems for monitoring produce movement from farm to market, or is the supply chain tracked manually? *This determines whether we're evaluating first deployments or enhancements to existing infrastructure.*
>
> 2. **Geographic scope**: Should we focus on specific regions or the entire national supply chain? Ghana's connectivity infrastructure varies significantly between Accra and northern regions. *This affects whether we can recommend cloud-dependent solutions or need offline-capable options.*
>
> 3. **Prior initiatives**: Has the ministry attempted technology solutions for post-harvest loss before? If so, what happened? *Understanding prior attempts helps us avoid recommending approaches that have already been tried and helps us anticipate institutional resistance.*
>
> 4. **Budget context**: Is this funded through the ministry's budget, a donor program, or a combination? *Donor-funded projects often have specific procurement and reporting requirements that constrain technology choices.*
>
> 5. **Decision scope**: Is the Permanent Secretary looking for a single recommended solution, or a shortlist of options with trade-offs? *This shapes whether our deliverable is a recommendation memo or an options paper.*

**Step 3 (combined with Step 4 intro):**
> Since this is a government ministry, I'll set the sensitivity tier to **Sensitive** -- I'll use [Client] placeholders in all artifacts and restrict distribution to the engagement team. Let me know if you'd prefer Open or Restricted instead.
>
> Here's the draft engagement brief:

---

## Example 2: Financial Services Due Diligence

**Consultant input:**
> "A regional bank in Southeast Asia is considering adopting a cloud-based core banking platform from a fintech vendor. They want us to do due diligence before making a $2M commitment. The CTO is driving this but the board needs to approve. They're currently on an on-premise legacy system that's 15 years old."

**Step 2 response:**
> Here are my clarification questions -- skip any that aren't answerable right now.
>
> 1. **Regulatory jurisdiction**: Which country is the bank headquartered in? Banking regulations and data localization requirements differ significantly across Southeast Asian jurisdictions. *This determines which regulatory frameworks we need to evaluate the vendor against.*
>
> 2. **Vendor identification**: Can you name the fintech vendor, or should we keep this anonymized? *If we know the vendor, we can do targeted due diligence. If not, we'll structure the assessment as a framework they can apply to any vendor.*
>
> 3. **Migration scope**: Is the bank planning to migrate all operations to the cloud platform at once, or a phased approach starting with specific product lines? *The migration strategy affects risk assessment -- a big-bang migration carries different risk than a phased rollout.*
>
> 4. **Board timeline**: When does the board expect to make this decision? Is there a contract deadline or competitive pressure driving urgency? *This determines how deep our assessment can go within the available time.*
>
> 5. **IT team capacity**: How large is the bank's IT team, and do they have cloud operations experience? *A legacy on-premise team moving to cloud for the first time will face a significant skills transition that affects implementation risk.*

---

## Example 3: Minimal Input Handling

**Consultant input:**
> "Blockchain for land records in Kenya."

**Step 2 response:**
> That gives me a starting point. Let me ask a few questions to shape the engagement brief.
>
> 1. **Client**: Who is the client -- the Ministry of Lands, a county government, a donor organization, or another entity? *This determines the sensitivity tier, procurement constraints, and who the deliverables serve.*
>
> 2. **Current state**: How are land records currently managed in the target jurisdiction -- paper-based, a digital registry, or a hybrid? *The baseline determines whether we're evaluating a first digitization or a technology upgrade.*
>
> 3. **Scope of evaluation**: Should we evaluate whether blockchain is the right approach for land records, or has that decision been made and you need a specific platform recommendation? *This determines whether we do a broad technology assessment or a focused vendor evaluation.*
>
> 4. **Stakeholders**: Who will receive and act on the recommendation -- a technical committee, a permanent secretary, a donor, or multiple audiences? *This shapes the deliverable format and depth.*
>
> 5. **Known constraints**: Are there any budget, timeline, or regulatory constraints I should know about upfront? *Even rough parameters help us calibrate the research depth and ROI analysis.*
